head(spam)
plot(density(spam$your[spam$type == "nonspam"]),
col = "blue", main = "", xlab = "Frequence of 'your'")
lines(density(spam$your[spam$type == "spam"]), col = "red")
abline(v = 0.5, col = "black")
prediction <- ifelse(spam$your > 0.5, "spam", "nonspam")
table(prediction, spam$type) / length(spam$type)
# In and out of sample errors
library(kernlab); data(spam); set.seed(333)
smallSpam <- spam[sample(dim(spam)[1], size = 10),]
spamLabel <- (smallSpam$type == "spam") * 1 + 1
plot(smallSpam$capitalAve, col = spamLabel)
rule1 <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 2.7] <- "spam"
prediction[x < 2.40] <- "nonspam"
prediction[(x >= 2.40 & x <= 2.45)] <- "spam"
prediction[(x > 2.45 & x <= 2.70)] <- "nonspam"
return(prediction)
}
table(rule1(smallSpam$capitalAve),smallSpam$type)
rule2 <- function(x){
prediction <- rep(NA,length(x))
prediction[x > 2.8] <- "spam"
prediction[x <= 2.8] <- "nonspam"
return(prediction)
}
table(rule2(smallSpam$capitalAve),smallSpam$type)
table(rule1(spam$capitalAve), spam$type)
table(rule2(spam$capitalAve), spam$type)
sum(rule1(spam$capitalAve) == spam$type)
sum(rule2(spam$capitalAve) == spam$type)
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type,
p = 0.75, list = FALSE)
inTrain
class(inTrain)
dim(inTrain)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
dim(training)
set.seed(32343)
if(require("e1071") == FALSE) install.packages("e1071")
modelFit <- train(type ~ ., data = training, method = "glm")
modelFit
dim(training)
modelFit$finalModel
predictions <- predict(modelFit, newdata = testing)
predictions
confusionMatrix(predictions, testing$type)
## Data slicing
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type,
p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
dim(training)
# Return training set
set.seed(32323)
folds <- createFolds(y = spam$type, k = 10, list = TRUE,
returnTrain = TRUE)
?createFolds
set.seed(32323)
folds <- createFolds(y = spam$type, k = 10, list = TRUE,
returnTrain = FALSE)
folds
class(folds)
length(folds)
sapply(folds, length)
folds[[1]][1:10]
# Resampling
set.seed(32323)
folds <- createResample(y = spam$type, times = 10, list = TRUE)
sapply(folds, length)
folds[[1]][1:10]
# Time slices
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y = tme, initialWindow = 20, horizon = 10)
names(folds)
folds$train[[1]]
class(folds)
length(folds)
folds[1]
class(folds[1])
length(folds[1])
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type,
p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
modelFit <- train(type ~ ., data = training, method = "glm")
args(train.default)
args(trainControl)
args(train.default)
args(trainControl)
set.seed(1235)
modelFit2 <- train(type ~ ., data = training, method = "glm")
modelFit2
## Plotting predictors
if(require(ISLR) == FALSE) install.packages("ISLR")
library(ISLR); library(ggplot2); library(caret)
data(Wage)
summary(Wage)
# Get training/test sets
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
dim(training); dim(testing)
str(Wage)
featurePlot(x = training[, c("age", "education", "jobclass")],
y = training$wage, plot = "pairs")
qplot(age, wage, data = training)
qplot(age, wage, colour = jobclass, data = training)
qq <- qplot(age, wage, colour = education, data = training)
qq + geom_smooth(method = "lm", formula = y ~ x)
qq <- qplot(age, wage, colour = education, data = training)
qq + geom_smooth(method = "lm", formula = y ~ x)
# cut2, making factors
if(require(Hmisc) == FALSE) install.packages("Hmisc")
library(Hmisc)
cutWage <- cut2(training$wage, g = 3)
table(cutWage)
class(cutWage)
cutWage
# Boxplots with cut2
p1 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot"))
p1 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot"))
p1
table(cutWage)
names(training)
age
training$age
unique(cutWage)
p2 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol = 2)
p2 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol = 2)
p1 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot"))
p1
# Boxplots with points overlayed
if(require(gridExtra) == FALSE) install.packages("gridExtra")
library(gridExtra)
p2 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol = 2)
# Tables
t1 <- table(cutWage, training$jobclass)
t1
head(cutWage)
head(training$jobclass)
cutWage
calss(cutWage)
class(cutWage)
t1
prop.table(t1, 1)
# Density plots
qplot(wage, colour = education, data = training, geom = "density")
qplot(wage)
qplot(wage, data = training)
hist(wage, data = training)
boxplot(wage, data = training)
boxplot(training$wage)
boxplot(training$wage, col = 3)
qplot(wage, colour = education, data = training, geom = "density")
qplot(wage, colour = education, data = training, geom = "histgram")
library(caret); library(kernlab); data(spam)
inTrain<- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
hist(training$capitalAve, main = "", xlab = "ave. capital run length")
library(caret); library(kernlab); data(spam)
inTrain<- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
hist(training$capitalAve, main = "", xlab = "ave. capital run length")
mean(training$capitalAve)
sd(training$capitalAve)
mean(training$capitalAve)
sd(training$capitalAve)
# Standardizing
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAve)
mean(trainCapAveS)
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
sd(trainCapAveS)
# Standardizing - test set, need to minus and divide by the training set, not test set
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(testCapAveS)
sd(testCapAveS)
# Standardizing - preProcess function
str(training)
preObj <- preProcess(training[, -58], method = c("center", "scale"))
str(preObj)
preObj
preObj <- preProcess(training[, -58], method = c("center", "scale"))
trainCapAveS <- predict(preObj, training[, -58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
aftPre <- predict(preObj, training[, -58])
str(aftPre)
str(training)
sd(training$make)
sd(aftPre$make)
mean(training$capitalAve)
mean(aftPre$capitalAve)
aftPreTest <- predict(preObj, testing[, -58])
mean(aftPreTest$capitalTotal)
mean(testing$capitalTotal)
sd(trainCapAveS)
testCapAves <- predict(preObj, testing[, -58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)
# Standardizing - preProcess argument
set.seed(32343)
modelFit <- train(type ~ ., data = training,
preProcess = c("center", "scale"), method = "glm")
modelFit
# Standardizing - Box-Cox transforms
preObj <- preProcess(training[,-58],method=c("BoxCox"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)
# Standardizing - Imputting data
set.seed(13343)
# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA
selectNA
# Impute and standardize
if(require(RANN) == FALSE) install.packages("RANN")
library(RANN)
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve
capAve
dim(training)
sum(selectNA)
dim(traininig)
dim(traininig)
dim(training)
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
quantile(capAve - capAveTruth)
quantile(capAve - capAveTruth)[selectNA]
quantile(capAve - capAveTruth)[!selectNA]
capAve[selectNA]
capAveTruth[selectNA]
quantile(capAve - capAveTruth)[selectNA]
(capAve - capAveTruth)[selectNA]
quantaile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[selectNA])
quantile(capAve - capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
## Covariate creation
library(kernlab);data(spam)
spam$capitalAveSq <- spam$capitalAve^2
library(ISLR); library(caret); data(Wage);
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass, data = training)
head(predict(dummies, newdata = training))
names(training)
unique(training$jobclass)
dummies
print(dummies)
dummies$vars
dummies$fullRank
head(predict(dummies, newdata = training))
# Removing zero covariates
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
# Spline basis
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
# Spline basis
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
# Fitting curves with splines
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex = 0.5)
points(training$age, predict(lm1, newdata = training), col = "red", pch = 19, cex = 0.5)
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex = 0.5)
points(training$age, predict(lm1, newdata = training), col = "red", pch = 19, cex = 0.5)
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
# Fitting curves with splines
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex = 0.5)
points(training$age, predict(lm1, newdata = training), col = "red", pch = 19, cex = 0.5)
# Splines on the test set
predict(bsBasis, age = testing$age)
## Preprocessing with principal components analysis
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
M
diM9traidinig
dim(training)
cor(training[, -58])
dim(cor(training[, -58]))
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
# Correlated predictors
names(spam)[c(34,32)]
plot(spam[,34],spam[,32])
# We could rotate the plot
X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857
plot(X, Y)
# Principal components in R - prcomp
smallSpam <- spam[,c(34,32)]
dim(smallSpam)
prComp <- prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])
prComp
prComp$rotation
# PCA on SPAM data
typeColor <- ((spam$type=="spam")*1 + 1)
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
prComp
prComp <- prcomp(log10(spam[,-58]+1))
dim(prComp)
preCOmp
prComp
prComp$x
dim(prComp$x)
names(prComp$x)
prComp$x[, 4]
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
preProc
preProc$x
preProc$x[, 1]
preProc$x[, 2]
print(preProc)
summary(preProc)
str(preProc)
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
# PCA with caret
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
spamPC
dim(spamPC)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
dim(spam)
dim(training)
dim(testing)
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
dim(trainPC)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
# Alternative (sets # of PCs)
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", data = training)
confusionMatrix(testing$type, predict(modelFit, testing))
modelFit$finalModel
dim(training)
modelFit
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", data = training)
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
modelFit
modelFit$finalModel
confusionMatrix(testing$type,predict(modelFit,testPC))
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", data = training)
confusionMatrix(testing$type, predict(modelFit, testing))
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", data = training)
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", pcaComp = 2, data = training)
?train
## Predictions with Regression
# Example: Old faithful eruptions
library(caret);data(faithful); set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,
p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
dim(faithful)
head(faithful)
lm1 <- lm(eruptions ~ waiting, data = trainFaith)
summary(lm1)
# Model fit
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,lm1$fitted,lwd=3)
lm1$fitted.values
dim(lm1$fitted.values)
lenght(lm1$fitted.values)
length(lm1$fitted.values)
dim(trainFaith)
# Predict a new value
coef(lm1)[1] + coef(lm1)[2]*80
lm1
newdata <- data.frame(waiting=80)
predict(lm1,newdata)
coef(lm1)[1]
# Plot predictions - training and test
par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,predict(lm1),lwd=3)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)
# Get training set/test set erros
# Calculate RMSE on training
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
# Calculate RMSE on test
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))
dim(testFaith)
# Prediction intervals
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
pred1
ord <- order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",,col=c(1,2,2),lty = c(1,1,1), lwd=3)
ord <- order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",,col=c(1,2,2),lty = c(1,1,1), lwd=3)
# Same process with caret
modFit <- train(eruptions ~ waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)
modFit <- train(eruptions ~ waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)
## Predicting with regression, multiple covariates
# Example: predicting wages
library(ISLR); library(ggplot2); library(caret);
data(Wage); Wage <- subset(Wage,select=-c(logwage))
summary(Wage)
# Get training/test sets
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
dim(training); dim(testing)
# Feature plot
featurePlot(x=training[,c("age","education","jobclass")],
y = training$wage,
plot="pairs")
# Plot age versus wage
qplot(age, wage, data = training)
# Plot age versus wage colour by jobclass
qplot(age, wage, colour = jobclass, data = training)
# plot age versus wage colour by education
qplot(age, wage, colour = education, data = training)
# Fit a linear model
modFit <- train(wage ~ age + jobclass + education, method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
# Diagnostics
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
# Color by variables not used in the model
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
# Plot by index
plot(finMod$residuals,pch=19)
# Predicted versus truth in test set
pred <- predict(modFit, testing)
qplot(wage,pred,colour=year,data=testing)
# Use all covariates
modFitAll<- train(wage ~ .,data=training,method="lm")
pred <- predict(modFitAll, testing)
qplot(wage,pred,data=testing)
## Predicting with trees
# Example: Iris Data
data(iris); library(ggplot2); library(caret)
names(iris)
table(iris$Species)
# Create training and test sets
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
# Iris petal widths/sepal width
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
# Plot Tree
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
# Prettier plots
if(require(rattle) == FALSE) install.packages("rattle")
