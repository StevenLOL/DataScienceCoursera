# Plot age versus wage
qplot(age, wage, data = training)
# Plot age versus wage colour by jobclass
qplot(age, wage, colour = jobclass, data = training)
# plot age versus wage colour by education
qplot(age, wage, colour = education, data = training)
# Fit a linear model
modFit <- train(wage ~ age + jobclass + education, method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
# Diagnostics
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
# Color by variables not used in the model
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
# Plot by index
plot(finMod$residuals,pch=19)
# Predicted versus truth in test set
pred <- predict(modFit, testing)
qplot(wage,pred,colour=year,data=testing)
# Use all covariates
modFitAll<- train(wage ~ .,data=training,method="lm")
pred <- predict(modFitAll, testing)
qplot(wage,pred,data=testing)
## Predicting with trees
# Example: Iris Data
data(iris); library(ggplot2); library(caret)
names(iris)
table(iris$Species)
# Create training and test sets
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
# Iris petal widths/sepal width
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
# Plot Tree
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
# Prettier plots
if(require(rattle) == FALSE) install.packages("rattle")
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
library(caret)
head(vowel.test)
head(vowel.train)
max(vowel.test$y)
rfMod <- train(y ~ ., data = vowel.train, method = "rf")
gbmMod <- train(y ~ ., data = vowel.train, method = "gbm")
set.seed(33833)
rfMod <- train(y ~ ., data = vowel.train, method = "rf")
gbmMod <- train(y ~ ., data = vowel.train, method = "gbm", verbose = FALSE)
rfPre <- predict(rfMod, volwel.test[,-1])
rfPre <- predict(rfMod, vowel.test[,-1])
gbmPre <- predict(gbmMod, vowel.test[, -1])
rfPre
str(vowel.train)
vowel.test$y <- as.factor(vowel.test$y)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
library(caret)
set.seed(33833)
rfMod <- train(y ~ ., data = vowel.train, method = "rf")
gbmMod <- train(y ~ ., data = vowel.train, method = "gbm", verbose = FALSE)
rfPre <- predict(rfMod, vowel.test[,-1])
gbmPre <- predict(gbmMod, vowel.test[, -1])
rfPre
confusionMatrix(rfPre, vowel.test$y)
confusionMatrix(rfPre, vowel.test$y)
confusionMatrix(gbmPre, vowel.test$y)
set.seed(33833)
gbmMod <- train(y ~ ., data = vowel.train, method = "gbm", verbose = FALSE)
rfPre <- predict(rfMod, vowel.test[,-1])
gbmPre <- predict(gbmMod, vowel.test[, -1])
confusionMatrix(rfPre, vowel.test$y)
confusionMatrix(gbmPre, vowel.test$y)
library(caret)
set.seed(33833)
rfMod <- train(y ~ ., data = vowel.train, method = "rf")
set.seed(33833)
gbmMod <- train(y ~ ., data = vowel.train, method = "gbm", verbose = FALSE)
rfPre <- predict(rfMod, vowel.test[,-1])
gbmPre <- predict(gbmMod, vowel.test[, -1])
confusionMatrix(rfPre, vowel.test$y)
confusionMatrix(gbmPre, vowel.test$y)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
inBuild <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage,
p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]
dim(training)
dim(testing)
dim(validation)
# Building two different models
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",
data=training,
trControl = trainControl(method="cv"),number=3)
# Predict on the testing set
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
preDF <- data.frame(rfPre, gbmPre, y = vowel.test$y)
combMod <- train(y ~ ., data = preDF, method = "gam")
combPre <- predict(combMod, preDF)
confusionMatrix(combPre, vowel.test$y)
confusionMatrix(rfPre, vowel.test$y)
confusionMatrix(gbmPre, vowel.test$y)
preDF <- data.frame(rfPre, gbmPre, y = vowel.test$y)
head(preDF)
combPre <- predict(combMod, preDF[, -3])
confusionMatrix(combPre, vowel.test$y)
head(preDF)
head(preDF, 10)
combPre <- predict(combMod, preDF)
head(combPre)
combMod <- train(y ~ ., data = preDF, method = "gam")
combPre <- predict(combMod, preDF)
confusionMatrix(combPre, vowel.test$y)
max(combPre)
comPre
combPre
names(preDF)
head(pred1)
head(pred2)
head(combPred)
combPred <- predict(combModFit,predDF)
predDF <- data.frame(pred1,pred2,wage=testing$wage)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
head(combPred)
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))
head(training)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(training)
names(testing)
rfMod <- train(diagnosis ~ ., method = "rf", data = training)
gbmMod <- train(diagnosis ~ ., method = "gbm", data = training, verbose = FALSE)
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
inBuild <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage,
p=0.7, list=FALSE)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]
dim(training)
dim(testing)
dim(validation)
# Building two different models
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",
data=training,
trControl = trainControl(method="cv"),number=3)
# Predict on the testing set
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
# Fit a model that combines predictors
predDF <- data.frame(pred1,pred2,wage=testing$wage)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
# Testing errors
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))
# Predict on validation data set
pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)
# Evaluate on validation
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)
# Evaluate on validation
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)
# Evaluate on validation
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
library(caret)
set.seed(33833)
rfMod <- train(y ~ ., data = vowel.train, method = "rf")
set.seed(33833)
gbmMod <- train(y ~ ., data = vowel.train, method = "gbm", verbose = FALSE)
rfPre <- predict(rfMod, vowel.test[,-1])
gbmPre <- predict(gbmMod, vowel.test[, -1])
confusionMatrix(rfPre, vowel.test$y)
confusionMatrix(gbmPre, vowel.test$y)
confusionMatrix(vowel.test$y, gbmPre$y)
confusionMatrix(vowel.test$y, gbmPre)
set.seed(33833)
rfMod <- train(y ~ ., data = vowel.train, method = "rf")
gbmMod <- train(y ~ ., data = vowel.train, method = "gbm", verbose = FALSE)
rfPre <- predict(rfMod, vowel.test[,-1])
gbmPre <- predict(gbmMod, vowel.test[, -1])
confusionMatrix(rfPre, vowel.test$y)
confusionMatrix(gbmPre, vowel.test$y)
preDF <- data.frame(rfPre, gbmPre, y = vowel.test$y)
combMod <- train(y ~ ., data = preDF, method = "gam")
combPre <- predict(combMod, preDF)
confusionMatrix(combPre, vowel.test$y)
str(preDF)
predict(combMod, preDF)
combMod <- train(y ~ ., data = preDF, method = "gmb")
combPre <- predict(combMod, preDF)
confusionMatrix(combPre, vowel.test$y)
class(preDF)
preDF <- data.frame(rfPre, gbmPre, y = vowel.test$y)
combMod <- train(y ~ ., method = "gam", data = preDF)
combPre <- predict(combMod, preDF)
confusionMatrix(combPre, vowel.test$y)
rfPre
gbmPre
vowel.test$y
combMod
summary(combMod)
summary(preDF)
str(preDF)
preDF
t <- train(y ~ ., method = "rf", data = preDF)
predict(t, preDF)
t <- train(y ~ ., method = "gbm", data = preDF)
predict(t, preDF)
t <- train(y ~ ., method = "gam", data = preDF)
predict(t, preDF)
t <- train(y ~ ., method = "gbm", data = preDF)
t <- train(y ~ ., method = "gbm", data = preDF, verbose = FALSE)
combMod <- train(y ~ ., method = "gbm", data = preDF)
combMod <- train(y ~ ., method = "gbm", data = preDF, verbose = FALSE)
combPre <- predict(combMod, preDF)
confusionMatrix(combPre, vowel.test$y)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
rfMod <- train(diagnosis ~ ., method = "rf", data = training)
set.seed(62433)
gbmMod <- train(diagnosis ~ ., method = "gbm", data = training, verbose = FALSE)
set.seed(62433)
ldaMod <- train(diagnosis ~ ., method = "lda", data = training)
rfPred <- predict(rfMod, data = testing[, -1])
gbmPred <- predict(gbmMod, data = testing[, -1])
ldaPred <- predict(ldaMod, data = testing[, -1])
combDF <- data.frame(rePred, gbmPred, ldaPred, diagnosis = testing$diagnosis)
set.seed(62433)
rfPred <- predict(rfMod, data = testing[, -1])
names(testing)
names(testing)
str(testing)
str(training)
set.seed(62433)
rfMod <- train(diagnosis ~ ., method = "rf", data = training)
names(testing[, -1])
rfPred <- predict(rfMod, data = testing[, -1])
rfPred <- predict(rfMod, testing[, -1])
set.seed(62433)
gbmMod <- train(diagnosis ~ ., method = "gbm", data = training, verbose = FALSE)
set.seed(62433)
ldaMod <- train(diagnosis ~ ., method = "lda", data = training)
gbmPred <- predict(gbmMod, testing[, -1])
ldaPred <- predict(ldaMod, testing[, -1])
combDF <- data.frame(rfPred, gbmPred, ldaPred, diagnosis = testing$diagnosis)
set.seed(62433)
combMod <- train(diagnosis ~., method = "rf", data = combDF)
combPred <- predict(combMod, combDF)
confusionMatrix(rfPred, testing$diagnosis)
confusionMatrix(gbmPred, testing$diagnosis)
confusionMatrix(ldaPred, testing$diagnosis)
confusionMatrix(combPred, testing$diagnosis)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(glmnet)
if(require(glmnet) == FALSE) install.packages("glmnet")
library(glmnet)
names(training)
head(training)
training[, -CompressiveStrength]
training[, -9]
head(training[, -9])
fit <- cv.glmnet(training$CompressiveStrength, training[, -9], family = "gaussian", alpha = 1)
fit <- glmnet(training$CompressiveStrength, training[, -9], family = "gaussian", alpha = 1)
training$CompressiveStrength
fit <- glmnet(training[, -9], training$CompressiveStrength, family = "gaussian", alpha = 1)
getwd()
setwd("./Chapter8.Practical_Machine_Learning/")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv", "gaData.csv", method = "curl")
getwd()
library(lubridate)  # For year() function below
if(require(lubridate) == FALSE) install.packages("lubridate")
library(lubridate)  # For year() function below
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
head(training)
head(testing)
head(tstrain)
library(forecast)
if(require(forecast) == FALSE) install.packages("forecast")
library(forecast)
?bats
head(tstrain)
head(testing)
head(training)
batman()
dim(training)
dim(testing)
batsFit <- bats(training)
data("USAccDeaths")
head(USAccDeaths)
USAccDeaths
tstrain
dim(training)
dim(testing)
dim(tstrain)
head(training)
head(testing)
training$visitsTumblr
testing$visitsTumblr
tstrain
class(tstrain)
str(training)
batsFit <- bats(tstrain)
plot(forecast(batsFit))
?forecast
dim(testing)
dim(tstrain)
length(tstrain)
forecast(batsFit)
plot(forecast(batsFit))
plot(forecast(batsFit), 300)
plot(forecast(batsFit), 3000)
forecast(batsFit), 3000
forecast(batsFit, 300)
testing
dim(testing)
forecast(batsFit, 235)
names(forecast(batsFit, 235))
forecast(batsFit, 235)$upper
forecast(batsFit, 235)$upper[,2]
forecast(batsFit, 235)$upper[,2] - testing$visitsTumblr
sum(forecast(batsFit, 235)$upper[,2] - testing$visitsTumblr < 0)
9/235
batsFit <- bats(tstrain)
sum(forecast(batsFit, 235)$upper[,2] - testing$visitsTumblr < 0)
sum(forecast(batsFit, 235)$upper[,2] - testing$visitsTumblr < 0) / 235
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(e1071)
if(require(e1071) == FALSE) install.packages("e1071")
library(e1071)
set.seed(325)
head(training)
head(testing)
set.seed(325)
svmMod <- svm(training[, -9], training$CompressiveStrength)
svmMod
svmPred <- predict(svmMod, testing[, -9])
svmPred
testing$CompressiveStrength
sqrt(sum(svmPred - testing$CompressiveStrength)*2))
sqrt(sum(svmPred - testing$CompressiveStrength)*2)
svmPred - testing$CompressiveStrength
(vmPred - testing$CompressiveStrength)*2
(svmPred - testing$CompressiveStrength)*2
(svmPred - testing$CompressiveStrength)
dim(testing)
(svmPred - testing$CompressiveStrength)*2
(svmPred - testing$CompressiveStrength)^2
sqrt(sum(svmPred - testing$CompressiveStrength)^2)
length((svmPred - testing$CompressiveStrength)^2)
dim(testing)
sum((svmPred - testing$CompressiveStrength)^2)
s(svmPred - testing$CompressiveStrength)
(svmPred - testing$CompressiveStrength)
set.seed(325)
svmMod <- svm(training[, -9], training$CompressiveStrength)
svmPred <- predict(svmMod, testing[, -9])
sum((svmPred - testing$CompressiveStrength)^2)
sqrt(sum((svmPred - testing$CompressiveStrength)^2))
set.seed(325)
svmMod <- svm(training[, -9], training$CompressiveStrength)
svmPred <- predict(svmMod, testing[, -9])
sqrt(sum((svmPred - testing$CompressiveStrength)^2))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
if(require(e1071) == FALSE) install.packages("e1071")
library(e1071)
set.seed(325)
svmMod <- svm(training[, -9], training$CompressiveStrength)
svmPred <- predict(svmMod, testing[, -9])
sqrt(sum((svmPred - testing$CompressiveStrength)^2))
RF 0.6082, GBM 0.5152, AG 0.6361
dim(testing)
sqrt(sum((svmPred - testing$CompressiveStrength)^2)/256)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
if(require(glmnet) == FALSE) install.packages("glmnet")
library(glmnet)
set.seed(233)
fit <- glmnet(training[, -9], training$CompressiveStrength, family = "gaussian", alpha = 1)
library(glmnet)
return <- matrix(ret.ff.zoo[which(index(ret.ff.zoo)==beta.df$date[2]), ])
data   <- matrix(unlist(beta.df[which(beta.df$date==beta.df$date[2]), ][ ,-1]),
ncol=num.factors)
?plot.enet
fit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
lassoMod <- train(CompressiveStrength, method = "lasso", data = training)
lassoMod <- train(CompressiveStrength ~ ., method = "lasso", data = training)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
lassoMod <- train(CompressiveStrength ~ ., method = "lasso", data = training)
lassoMod$finalModel
plot(lassoMod$finalModel)
plot.enet
sqrt(sum((svmPred - testing$CompressiveStrength)^2)/256)
plot(lassoMod)
plot.enet
lassoMod <- train(CompressiveStrength ~ ., method = "lasso", data = training)
plot(lassoMod)
plot.enet
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
lassoMod <- train(CompressiveStrength ~ ., method = "lasso", data = training)
plot(lassoMod)
plot.enet
plot.enet(lassoMod)
plot.enet
plot(lassoMod)
lassoMod$finalModel
plot(lassoMod$finalModel)
plot(lassoMod$finalModel)
