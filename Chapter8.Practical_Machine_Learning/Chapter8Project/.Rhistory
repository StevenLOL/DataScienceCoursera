length(folds[1])
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type,
p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
modelFit <- train(type ~ ., data = training, method = "glm")
args(train.default)
args(trainControl)
args(train.default)
args(trainControl)
set.seed(1235)
modelFit2 <- train(type ~ ., data = training, method = "glm")
modelFit2
## Plotting predictors
if(require(ISLR) == FALSE) install.packages("ISLR")
library(ISLR); library(ggplot2); library(caret)
data(Wage)
summary(Wage)
# Get training/test sets
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
dim(training); dim(testing)
str(Wage)
featurePlot(x = training[, c("age", "education", "jobclass")],
y = training$wage, plot = "pairs")
qplot(age, wage, data = training)
qplot(age, wage, colour = jobclass, data = training)
qq <- qplot(age, wage, colour = education, data = training)
qq + geom_smooth(method = "lm", formula = y ~ x)
qq <- qplot(age, wage, colour = education, data = training)
qq + geom_smooth(method = "lm", formula = y ~ x)
# cut2, making factors
if(require(Hmisc) == FALSE) install.packages("Hmisc")
library(Hmisc)
cutWage <- cut2(training$wage, g = 3)
table(cutWage)
class(cutWage)
cutWage
# Boxplots with cut2
p1 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot"))
p1 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot"))
p1
table(cutWage)
names(training)
age
training$age
unique(cutWage)
p2 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol = 2)
p2 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol = 2)
p1 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot"))
p1
# Boxplots with points overlayed
if(require(gridExtra) == FALSE) install.packages("gridExtra")
library(gridExtra)
p2 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot", "jitter"))
grid.arrange(p1, p2, ncol = 2)
# Tables
t1 <- table(cutWage, training$jobclass)
t1
head(cutWage)
head(training$jobclass)
cutWage
calss(cutWage)
class(cutWage)
t1
prop.table(t1, 1)
# Density plots
qplot(wage, colour = education, data = training, geom = "density")
qplot(wage)
qplot(wage, data = training)
hist(wage, data = training)
boxplot(wage, data = training)
boxplot(training$wage)
boxplot(training$wage, col = 3)
qplot(wage, colour = education, data = training, geom = "density")
qplot(wage, colour = education, data = training, geom = "histgram")
library(caret); library(kernlab); data(spam)
inTrain<- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
hist(training$capitalAve, main = "", xlab = "ave. capital run length")
library(caret); library(kernlab); data(spam)
inTrain<- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
hist(training$capitalAve, main = "", xlab = "ave. capital run length")
mean(training$capitalAve)
sd(training$capitalAve)
mean(training$capitalAve)
sd(training$capitalAve)
# Standardizing
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAve)
mean(trainCapAveS)
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
sd(trainCapAveS)
# Standardizing - test set, need to minus and divide by the training set, not test set
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(testCapAveS)
sd(testCapAveS)
# Standardizing - preProcess function
str(training)
preObj <- preProcess(training[, -58], method = c("center", "scale"))
str(preObj)
preObj
preObj <- preProcess(training[, -58], method = c("center", "scale"))
trainCapAveS <- predict(preObj, training[, -58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
aftPre <- predict(preObj, training[, -58])
str(aftPre)
str(training)
sd(training$make)
sd(aftPre$make)
mean(training$capitalAve)
mean(aftPre$capitalAve)
aftPreTest <- predict(preObj, testing[, -58])
mean(aftPreTest$capitalTotal)
mean(testing$capitalTotal)
sd(trainCapAveS)
testCapAves <- predict(preObj, testing[, -58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)
# Standardizing - preProcess argument
set.seed(32343)
modelFit <- train(type ~ ., data = training,
preProcess = c("center", "scale"), method = "glm")
modelFit
# Standardizing - Box-Cox transforms
preObj <- preProcess(training[,-58],method=c("BoxCox"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)
# Standardizing - Imputting data
set.seed(13343)
# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA
selectNA
# Impute and standardize
if(require(RANN) == FALSE) install.packages("RANN")
library(RANN)
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve
capAve
dim(training)
sum(selectNA)
dim(traininig)
dim(traininig)
dim(training)
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
quantile(capAve - capAveTruth)
quantile(capAve - capAveTruth)[selectNA]
quantile(capAve - capAveTruth)[!selectNA]
capAve[selectNA]
capAveTruth[selectNA]
quantile(capAve - capAveTruth)[selectNA]
(capAve - capAveTruth)[selectNA]
quantaile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[selectNA])
quantile(capAve - capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
## Covariate creation
library(kernlab);data(spam)
spam$capitalAveSq <- spam$capitalAve^2
library(ISLR); library(caret); data(Wage);
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass, data = training)
head(predict(dummies, newdata = training))
names(training)
unique(training$jobclass)
dummies
print(dummies)
dummies$vars
dummies$fullRank
head(predict(dummies, newdata = training))
# Removing zero covariates
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
# Spline basis
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
# Spline basis
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
# Fitting curves with splines
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex = 0.5)
points(training$age, predict(lm1, newdata = training), col = "red", pch = 19, cex = 0.5)
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex = 0.5)
points(training$age, predict(lm1, newdata = training), col = "red", pch = 19, cex = 0.5)
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
# Fitting curves with splines
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex = 0.5)
points(training$age, predict(lm1, newdata = training), col = "red", pch = 19, cex = 0.5)
# Splines on the test set
predict(bsBasis, age = testing$age)
## Preprocessing with principal components analysis
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
M
diM9traidinig
dim(training)
cor(training[, -58])
dim(cor(training[, -58]))
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
# Correlated predictors
names(spam)[c(34,32)]
plot(spam[,34],spam[,32])
# We could rotate the plot
X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857
plot(X, Y)
# Principal components in R - prcomp
smallSpam <- spam[,c(34,32)]
dim(smallSpam)
prComp <- prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])
prComp
prComp$rotation
# PCA on SPAM data
typeColor <- ((spam$type=="spam")*1 + 1)
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
prComp
prComp <- prcomp(log10(spam[,-58]+1))
dim(prComp)
preCOmp
prComp
prComp$x
dim(prComp$x)
names(prComp$x)
prComp$x[, 4]
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
preProc
preProc$x
preProc$x[, 1]
preProc$x[, 2]
print(preProc)
summary(preProc)
str(preProc)
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
# PCA with caret
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
spamPC
dim(spamPC)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
dim(spam)
dim(training)
dim(testing)
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
dim(trainPC)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
# Alternative (sets # of PCs)
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", data = training)
confusionMatrix(testing$type, predict(modelFit, testing))
modelFit$finalModel
dim(training)
modelFit
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", data = training)
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
modelFit
modelFit$finalModel
confusionMatrix(testing$type,predict(modelFit,testPC))
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", data = training)
confusionMatrix(testing$type, predict(modelFit, testing))
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", data = training)
modelFit <- train(training$type ~ ., method = "glm", preProcess = "pca", pcaComp = 2, data = training)
?train
## Predictions with Regression
# Example: Old faithful eruptions
library(caret);data(faithful); set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,
p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
dim(faithful)
head(faithful)
lm1 <- lm(eruptions ~ waiting, data = trainFaith)
summary(lm1)
# Model fit
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,lm1$fitted,lwd=3)
lm1$fitted.values
dim(lm1$fitted.values)
lenght(lm1$fitted.values)
length(lm1$fitted.values)
dim(trainFaith)
# Predict a new value
coef(lm1)[1] + coef(lm1)[2]*80
lm1
newdata <- data.frame(waiting=80)
predict(lm1,newdata)
coef(lm1)[1]
# Plot predictions - training and test
par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,predict(lm1),lwd=3)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)
# Get training set/test set erros
# Calculate RMSE on training
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
# Calculate RMSE on test
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))
dim(testFaith)
# Prediction intervals
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
pred1
ord <- order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",,col=c(1,2,2),lty = c(1,1,1), lwd=3)
ord <- order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",,col=c(1,2,2),lty = c(1,1,1), lwd=3)
# Same process with caret
modFit <- train(eruptions ~ waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)
modFit <- train(eruptions ~ waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)
## Predicting with regression, multiple covariates
# Example: predicting wages
library(ISLR); library(ggplot2); library(caret);
data(Wage); Wage <- subset(Wage,select=-c(logwage))
summary(Wage)
# Get training/test sets
inTrain <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
dim(training); dim(testing)
# Feature plot
featurePlot(x=training[,c("age","education","jobclass")],
y = training$wage,
plot="pairs")
# Plot age versus wage
qplot(age, wage, data = training)
# Plot age versus wage colour by jobclass
qplot(age, wage, colour = jobclass, data = training)
# plot age versus wage colour by education
qplot(age, wage, colour = education, data = training)
# Fit a linear model
modFit <- train(wage ~ age + jobclass + education, method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
# Diagnostics
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
# Color by variables not used in the model
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
# Plot by index
plot(finMod$residuals,pch=19)
# Predicted versus truth in test set
pred <- predict(modFit, testing)
qplot(wage,pred,colour=year,data=testing)
# Use all covariates
modFitAll<- train(wage ~ .,data=training,method="lm")
pred <- predict(modFitAll, testing)
qplot(wage,pred,data=testing)
## Predicting with trees
# Example: Iris Data
data(iris); library(ggplot2); library(caret)
names(iris)
table(iris$Species)
# Create training and test sets
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
# Iris petal widths/sepal width
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
# Plot Tree
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
# Prettier plots
if(require(rattle) == FALSE) install.packages("rattle")
# load required packages
library(kernlab)
if(require(tree) == FALSE) install.packages("tree")
library(tree)
library(randomForest)
if(require(beepr) == FALSE) install.packages("beepr")
library(beepr) # try it! beep()
if(require(pROC) == FALSE) install.packages("pROC")
library(pROC)
# load dataset
data(spam)
# take a look
str(spam)
# take a look
str(spam)
#View(spam)
# preparation for cross validation:
# split the dataset into 2 halves,
# 2300 samples for training and 2301 for testing
num.samples <- nrow(spam) # 4,601
num.train   <- round(num.samples/2) # 2,300
num.test    <- num.samples - num.train # 2,301
num.var     <- ncol(spam) # 58
# set up the indices
idx       <- sample(1:num.samples)
set.seed(150715)
train.idx <- idx[seq(num.train)]
test.idx  <- setdiff(idx,train.idx)
# subset the data
spam.train <- spam[train.idx,]
spam.test  <- spam[test.idx,]
# take a quick look
str(spam.train)
str(spam.test)
table(spam.train$type)
table(spam.test$type)
tree.mod <- tree(type ~ ., data = spam.train)
rf.mod <- randomForest(type ~ ., data = spam.train,
mtry = floor(sqrt(num.var - 1)), # 7; only difference from bagging is here
ntree = 300,
proximity = TRUE,
importance = TRUE)
beep()
# Out-of-bag (OOB) error rate as a function of num. of trees:
plot(rf.mod$err.rate[,1], type = "l", lwd = 3, col = "blue",
main = "Random forest: OOB estimate of error rate",
xlab = "Number of Trees", ylab = "OOB error rate")
# tuning the mtry hyperparameter
tuneRF(subset(spam.train, select = -type),
spam.train$type,
ntreetry = 100)
title("Random forest: Tuning the mtry hyperparameter")
# variable importance
varImpPlot(rf.mod,
main = "Random forest: Variable importance")
training <- read.csv("pml_training.csv")
testing <- read.csv("pml_testing.csv")
# Take a glance at the data
dim(training)
dim(testing)
str(training)
str(testing)
# Download and prepare data
setwd("./Chapter8.Practical_Machine_Learning/Chapter8Project/")
training <- read.csv("pml_training.csv")
testing <- read.csv("pml_testing.csv")
# Take a glance at the data
dim(training)
dim(testing)
str(training)
str(testing)
table(testing)
table(testing$classe)
dim(testing)
table(training$classe)
table(testing)
summary(testing)
summary(training)
head(training)
view(training)
View(training)
table(training$num_window)
names(training)
str(training)
str(spam.train)
